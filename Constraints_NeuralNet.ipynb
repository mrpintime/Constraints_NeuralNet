{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWwTMv7LqtNDYsTR5rPhfK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrpintime/Constraints_NeuralNet/blob/main/Constraints_NeuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 7  \n"
      ],
      "metadata": {
        "id": "tAZ5NDO4Msj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approche 1  \n",
        "Several Conflict Matrix"
      ],
      "metadata": {
        "id": "0SLNJXfAKXKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "list_of_conflicts = []\n",
        "\n",
        "while len(list_of_conflicts) < 500:\n",
        "    pairs_list = set()\n",
        "    matrix = np.zeros((24, 24), dtype=int)\n",
        "\n",
        "    while matrix.sum() < 40:\n",
        "        num1 = np.random.choice(range(24))\n",
        "        num2 = np.random.choice(range(24))\n",
        "\n",
        "        if num1 == num2:\n",
        "            continue\n",
        "\n",
        "        pair = (num1, num2)\n",
        "        if pair in pairs_list:\n",
        "            continue\n",
        "\n",
        "        pairs_list.add(pair)\n",
        "        matrix[num1, num2] = 1\n",
        "\n",
        "    if not any(np.array_equal(matrix, conflict) for conflict in list_of_conflicts):\n",
        "        list_of_conflicts.append(matrix)"
      ],
      "metadata": {
        "id": "lLe8nwxLJzIt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conflicts = np.array(list_of_conflicts)"
      ],
      "metadata": {
        "id": "fvYvl0ID6xVR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conflicts.shape"
      ],
      "metadata": {
        "id": "MMMdGqWFcOSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53123775-90e9-4dae-b1b3-6889a2e57e77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 24, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adjacent_mask(n_seats, seats_per_row, seats_per_col):\n",
        "    adjacent_mask = np.zeros((n_seats, n_seats))\n",
        "    for i in range(n_seats):\n",
        "        if i % seats_per_row != 0:\n",
        "            adjacent_mask[i, i-1] = 1\n",
        "        if i % seats_per_row != seats_per_row-1:\n",
        "            adjacent_mask[i, i+1] = 1\n",
        "        if i >= seats_per_row:\n",
        "            adjacent_mask[i, i-seats_per_row] = 1\n",
        "        if i < n_seats-seats_per_row:\n",
        "            adjacent_mask[i, i+seats_per_row] = 1\n",
        "    return adjacent_mask\n",
        "\n",
        "adjacent_mask = create_adjacent_mask(24,6,4)"
      ],
      "metadata": {
        "id": "CcVuuOSDgQSe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adjacent_mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RCXexBzAU9l",
        "outputId": "48565a43-3b32-41ac-fde0-da28fe11bc19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow"
      ],
      "metadata": {
        "id": "isnIm152L3Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "adjacent_mask = create_adjacent_mask(24, 6, 4)\n",
        "\n",
        "def calculate_conflict(seating_arrangement, conflict_matrix):\n",
        "    ca_mul = tf.convert_to_tensor(conflict_matrix * adjacent_mask, tf.float64)\n",
        "    conflicts = tf.reduce_sum(tf.matmul(tf.cast(seating_arrangement, tf.float64), ca_mul))\n",
        "    return conflicts\n",
        "\n",
        "def custom_loss(predicted_seating_arrangement, conflicts_tensor):\n",
        "    loss = 0\n",
        "    batch_size = predicted_seating_arrangement.shape[0]\n",
        "\n",
        "    # Ensure the predicted seating arrangement is in float64\n",
        "    predicted_seating_arrangement = tf.cast(predicted_seating_arrangement, tf.float64)\n",
        "\n",
        "    conflict = calculate_conflict(predicted_seating_arrangement, conflicts_tensor)\n",
        "    # Ensure each seat is assigned to only one person (columns should sum to 1)\n",
        "    col_sum_loss = tf.reduce_sum((tf.reduce_sum(predicted_seating_arrangement, axis=1) - 1) ** 2)\n",
        "\n",
        "    loss = col_sum_loss + conflict\n",
        "\n",
        "    return loss / tf.cast(batch_size, tf.float64)\n",
        "\n",
        "conflicts_tensor = tf.convert_to_tensor(conflicts, tf.float64)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(conflicts_tensor.shape[1:]),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(80, activation='relu'),\n",
        "    tf.keras.layers.Dense(24*24),\n",
        "    tf.keras.layers.Reshape((24, 24)),\n",
        "    tf.keras.layers.Softmax(axis=2)  # Applying softmax along the last axis\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "for epoch in range(20):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predicted_seating_arrangement = model(conflicts_tensor, training=True)\n",
        "        loss = custom_loss(predicted_seating_arrangement, conflicts_tensor)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.numpy()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3jEOYY6bDiG",
        "outputId": "f3bff720-040a-4416-c5f9-63e4007c3b18",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 5.570847798779119\n",
            "Epoch: 1, Loss: 5.59418144604875\n",
            "Epoch: 2, Loss: 5.475330373890806\n",
            "Epoch: 3, Loss: 5.474966782635238\n",
            "Epoch: 4, Loss: 5.4444172224305625\n",
            "Epoch: 5, Loss: 5.387312345791366\n",
            "Epoch: 6, Loss: 5.305951939639843\n",
            "Epoch: 7, Loss: 5.212041281853652\n",
            "Epoch: 8, Loss: 5.132420952923065\n",
            "Epoch: 9, Loss: 5.069898444984496\n",
            "Epoch: 10, Loss: 5.000904045017689\n",
            "Epoch: 11, Loss: 4.947270622131727\n",
            "Epoch: 12, Loss: 4.884362623486902\n",
            "Epoch: 13, Loss: 4.829899376377765\n",
            "Epoch: 14, Loss: 4.77549751337528\n",
            "Epoch: 15, Loss: 4.733116954150112\n",
            "Epoch: 16, Loss: 4.694029573080971\n",
            "Epoch: 17, Loss: 4.660932104732362\n",
            "Epoch: 18, Loss: 4.631501168267163\n",
            "Epoch: 19, Loss: 4.6045304012002415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "for epoch in range(300):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predicted_seating_arrangement = model(conflicts_tensor, training=True)\n",
        "        loss = custom_loss(predicted_seating_arrangement, conflicts_tensor)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.numpy()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwr_60XkHjwm",
        "outputId": "d5b8321e-bc83-49c2-94f8-d548f27a34bf",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 4.579114785920096\n",
            "Epoch: 1, Loss: 5.893860540007823\n",
            "Epoch: 2, Loss: 4.70372560090229\n",
            "Epoch: 3, Loss: 4.926767641351857\n",
            "Epoch: 4, Loss: 4.986821002793053\n",
            "Epoch: 5, Loss: 4.916320792731844\n",
            "Epoch: 6, Loss: 4.891510845305187\n",
            "Epoch: 7, Loss: 4.894340311439324\n",
            "Epoch: 8, Loss: 4.894858945014623\n",
            "Epoch: 9, Loss: 4.883464680067839\n",
            "Epoch: 10, Loss: 4.861670090463209\n",
            "Epoch: 11, Loss: 4.833567498819177\n",
            "Epoch: 12, Loss: 4.801749139903426\n",
            "Epoch: 13, Loss: 4.767439760049987\n",
            "Epoch: 14, Loss: 4.73374975494317\n",
            "Epoch: 15, Loss: 4.704870889716452\n",
            "Epoch: 16, Loss: 4.68214811884431\n",
            "Epoch: 17, Loss: 4.665638914285193\n",
            "Epoch: 18, Loss: 4.65194671578651\n",
            "Epoch: 19, Loss: 4.635906267330122\n",
            "Epoch: 20, Loss: 4.618849898924933\n",
            "Epoch: 21, Loss: 4.601255531385126\n",
            "Epoch: 22, Loss: 4.585365675824594\n",
            "Epoch: 23, Loss: 4.573529364328061\n",
            "Epoch: 24, Loss: 4.558374694936577\n",
            "Epoch: 25, Loss: 4.542886721202614\n",
            "Epoch: 26, Loss: 4.530422788371008\n",
            "Epoch: 27, Loss: 4.520319560036472\n",
            "Epoch: 28, Loss: 4.511526895646733\n",
            "Epoch: 29, Loss: 4.501539661518953\n",
            "Epoch: 30, Loss: 4.493399327319307\n",
            "Epoch: 31, Loss: 4.486258443003932\n",
            "Epoch: 32, Loss: 4.478473223915873\n",
            "Epoch: 33, Loss: 4.4722841393279955\n",
            "Epoch: 34, Loss: 4.465942008631682\n",
            "Epoch: 35, Loss: 4.459861499299409\n",
            "Epoch: 36, Loss: 4.454534676562578\n",
            "Epoch: 37, Loss: 4.449140589382558\n",
            "Epoch: 38, Loss: 4.443952700561428\n",
            "Epoch: 39, Loss: 4.439157509142086\n",
            "Epoch: 40, Loss: 4.43440228347026\n",
            "Epoch: 41, Loss: 4.429667081823976\n",
            "Epoch: 42, Loss: 4.425373858119842\n",
            "Epoch: 43, Loss: 4.421216236959246\n",
            "Epoch: 44, Loss: 4.417550371923865\n",
            "Epoch: 45, Loss: 4.414131903972872\n",
            "Epoch: 46, Loss: 4.4107050393700575\n",
            "Epoch: 47, Loss: 4.407618098471293\n",
            "Epoch: 48, Loss: 4.404847530229643\n",
            "Epoch: 49, Loss: 4.402029048244969\n",
            "Epoch: 50, Loss: 4.399074119837482\n",
            "Epoch: 51, Loss: 4.396562267076047\n",
            "Epoch: 52, Loss: 4.394042938349228\n",
            "Epoch: 53, Loss: 4.391398862964375\n",
            "Epoch: 54, Loss: 4.3892393093818125\n",
            "Epoch: 55, Loss: 4.387095568332509\n",
            "Epoch: 56, Loss: 4.38499860567542\n",
            "Epoch: 57, Loss: 4.382994511773557\n",
            "Epoch: 58, Loss: 4.381199122844912\n",
            "Epoch: 59, Loss: 4.379460096904023\n",
            "Epoch: 60, Loss: 4.3776535127591005\n",
            "Epoch: 61, Loss: 4.376212317851036\n",
            "Epoch: 62, Loss: 4.37463585257357\n",
            "Epoch: 63, Loss: 4.37312576272733\n",
            "Epoch: 64, Loss: 4.371771161132249\n",
            "Epoch: 65, Loss: 4.3703781464431275\n",
            "Epoch: 66, Loss: 4.369057429854521\n",
            "Epoch: 67, Loss: 4.367867046383354\n",
            "Epoch: 68, Loss: 4.366673752958196\n",
            "Epoch: 69, Loss: 4.365546379856279\n",
            "Epoch: 70, Loss: 4.364457603261689\n",
            "Epoch: 71, Loss: 4.3633807438796435\n",
            "Epoch: 72, Loss: 4.36238109295914\n",
            "Epoch: 73, Loss: 4.361397083914987\n",
            "Epoch: 74, Loss: 4.360468075449693\n",
            "Epoch: 75, Loss: 4.3595790025771715\n",
            "Epoch: 76, Loss: 4.358705723080742\n",
            "Epoch: 77, Loss: 4.357876408876755\n",
            "Epoch: 78, Loss: 4.357060909524209\n",
            "Epoch: 79, Loss: 4.356276225811073\n",
            "Epoch: 80, Loss: 4.355512422386916\n",
            "Epoch: 81, Loss: 4.3547934589986355\n",
            "Epoch: 82, Loss: 4.35408975158421\n",
            "Epoch: 83, Loss: 4.3534290266065\n",
            "Epoch: 84, Loss: 4.3527988345419315\n",
            "Epoch: 85, Loss: 4.352210450163624\n",
            "Epoch: 86, Loss: 4.3517354271915245\n",
            "Epoch: 87, Loss: 4.3515530452948985\n",
            "Epoch: 88, Loss: 4.351769729053087\n",
            "Epoch: 89, Loss: 4.352522240376593\n",
            "Epoch: 90, Loss: 4.351005758566344\n",
            "Epoch: 91, Loss: 4.349137502828522\n",
            "Epoch: 92, Loss: 4.34878172610417\n",
            "Epoch: 93, Loss: 4.349446154630988\n",
            "Epoch: 94, Loss: 4.3494043100330035\n",
            "Epoch: 95, Loss: 4.348305075067813\n",
            "Epoch: 96, Loss: 4.347892445995465\n",
            "Epoch: 97, Loss: 4.347260338719792\n",
            "Epoch: 98, Loss: 4.346638298759712\n",
            "Epoch: 99, Loss: 4.346307056266784\n",
            "Epoch: 100, Loss: 4.345575302936768\n",
            "Epoch: 101, Loss: 4.345286005010831\n",
            "Epoch: 102, Loss: 4.345147380825118\n",
            "Epoch: 103, Loss: 4.344212568499863\n",
            "Epoch: 104, Loss: 4.34372864483335\n",
            "Epoch: 105, Loss: 4.34398599048614\n",
            "Epoch: 106, Loss: 4.3435093628309565\n",
            "Epoch: 107, Loss: 4.34264850603718\n",
            "Epoch: 108, Loss: 4.342531881629197\n",
            "Epoch: 109, Loss: 4.342543313640873\n",
            "Epoch: 110, Loss: 4.342060131671416\n",
            "Epoch: 111, Loss: 4.3416122926460305\n",
            "Epoch: 112, Loss: 4.341402884659457\n",
            "Epoch: 113, Loss: 4.341153930412018\n",
            "Epoch: 114, Loss: 4.340932738321298\n",
            "Epoch: 115, Loss: 4.340789409165348\n",
            "Epoch: 116, Loss: 4.340440826786315\n",
            "Epoch: 117, Loss: 4.340036561776018\n",
            "Epoch: 118, Loss: 4.339879416273025\n",
            "Epoch: 119, Loss: 4.339819823120157\n",
            "Epoch: 120, Loss: 4.3396007542492985\n",
            "Epoch: 121, Loss: 4.339309132722406\n",
            "Epoch: 122, Loss: 4.339111409076031\n",
            "Epoch: 123, Loss: 4.338952649088226\n",
            "Epoch: 124, Loss: 4.338824566537336\n",
            "Epoch: 125, Loss: 4.33883092262756\n",
            "Epoch: 126, Loss: 4.339094107212266\n",
            "Epoch: 127, Loss: 4.33938703472652\n",
            "Epoch: 128, Loss: 4.339798335358982\n",
            "Epoch: 129, Loss: 4.339380226465534\n",
            "Epoch: 130, Loss: 4.338614349804309\n",
            "Epoch: 131, Loss: 4.337686765250267\n",
            "Epoch: 132, Loss: 4.337506508406702\n",
            "Epoch: 133, Loss: 4.337929702889844\n",
            "Epoch: 134, Loss: 4.338141566418002\n",
            "Epoch: 135, Loss: 4.3378312549769875\n",
            "Epoch: 136, Loss: 4.337177754775958\n",
            "Epoch: 137, Loss: 4.336788907717218\n",
            "Epoch: 138, Loss: 4.336845205902201\n",
            "Epoch: 139, Loss: 4.336992190092485\n",
            "Epoch: 140, Loss: 4.336893993120139\n",
            "Epoch: 141, Loss: 4.336521778150231\n",
            "Epoch: 142, Loss: 4.336257281987296\n",
            "Epoch: 143, Loss: 4.336271837398628\n",
            "Epoch: 144, Loss: 4.336451963855537\n",
            "Epoch: 145, Loss: 4.336494593703116\n",
            "Epoch: 146, Loss: 4.3364770073371295\n",
            "Epoch: 147, Loss: 4.3365321692923064\n",
            "Epoch: 148, Loss: 4.337147747176161\n",
            "Epoch: 149, Loss: 4.338200672093952\n",
            "Epoch: 150, Loss: 4.339759499897197\n",
            "Epoch: 151, Loss: 4.339710835143408\n",
            "Epoch: 152, Loss: 4.338227940047559\n",
            "Epoch: 153, Loss: 4.336045908356601\n",
            "Epoch: 154, Loss: 4.3356274582005\n",
            "Epoch: 155, Loss: 4.336648530264145\n",
            "Epoch: 156, Loss: 4.336964816742754\n",
            "Epoch: 157, Loss: 4.336146220066357\n",
            "Epoch: 158, Loss: 4.335083495147229\n",
            "Epoch: 159, Loss: 4.335074823724923\n",
            "Epoch: 160, Loss: 4.33564812080041\n",
            "Epoch: 161, Loss: 4.335691728190936\n",
            "Epoch: 162, Loss: 4.335155104822408\n",
            "Epoch: 163, Loss: 4.3346328712212\n",
            "Epoch: 164, Loss: 4.334683708341063\n",
            "Epoch: 165, Loss: 4.334903183320533\n",
            "Epoch: 166, Loss: 4.334742431734881\n",
            "Epoch: 167, Loss: 4.334291445839477\n",
            "Epoch: 168, Loss: 4.33399217353999\n",
            "Epoch: 169, Loss: 4.3340828806357825\n",
            "Epoch: 170, Loss: 4.334267685123762\n",
            "Epoch: 171, Loss: 4.3342138803214745\n",
            "Epoch: 172, Loss: 4.333891463436736\n",
            "Epoch: 173, Loss: 4.333610888115449\n",
            "Epoch: 174, Loss: 4.333587196856393\n",
            "Epoch: 175, Loss: 4.333722799488114\n",
            "Epoch: 176, Loss: 4.333801990549113\n",
            "Epoch: 177, Loss: 4.333701526941422\n",
            "Epoch: 178, Loss: 4.333533188110239\n",
            "Epoch: 179, Loss: 4.333470640908136\n",
            "Epoch: 180, Loss: 4.333656154756141\n",
            "Epoch: 181, Loss: 4.3341433125788935\n",
            "Epoch: 182, Loss: 4.3350971592509735\n",
            "Epoch: 183, Loss: 4.336540313374403\n",
            "Epoch: 184, Loss: 4.338964291853337\n",
            "Epoch: 185, Loss: 4.340881584514513\n",
            "Epoch: 186, Loss: 4.34112016770586\n",
            "Epoch: 187, Loss: 4.337295594322558\n",
            "Epoch: 188, Loss: 4.333741731980452\n",
            "Epoch: 189, Loss: 4.334300496950367\n",
            "Epoch: 190, Loss: 4.336708371585583\n",
            "Epoch: 191, Loss: 4.3368970032455385\n",
            "Epoch: 192, Loss: 4.3344588993366875\n",
            "Epoch: 193, Loss: 4.333346429527973\n",
            "Epoch: 194, Loss: 4.334470806443033\n",
            "Epoch: 195, Loss: 4.335085800379399\n",
            "Epoch: 196, Loss: 4.334162526795622\n",
            "Epoch: 197, Loss: 4.333104272079738\n",
            "Epoch: 198, Loss: 4.333414989417957\n",
            "Epoch: 199, Loss: 4.334065780885589\n",
            "Epoch: 200, Loss: 4.333608555311867\n",
            "Epoch: 201, Loss: 4.332937387549013\n",
            "Epoch: 202, Loss: 4.332994057834465\n",
            "Epoch: 203, Loss: 4.3332971119476875\n",
            "Epoch: 204, Loss: 4.333160406197813\n",
            "Epoch: 205, Loss: 4.3327148189904605\n",
            "Epoch: 206, Loss: 4.332609584967771\n",
            "Epoch: 207, Loss: 4.332848473253034\n",
            "Epoch: 208, Loss: 4.332864534416205\n",
            "Epoch: 209, Loss: 4.33257259878919\n",
            "Epoch: 210, Loss: 4.332319850126585\n",
            "Epoch: 211, Loss: 4.332403533128425\n",
            "Epoch: 212, Loss: 4.332599928990263\n",
            "Epoch: 213, Loss: 4.3325171025119\n",
            "Epoch: 214, Loss: 4.332237448025455\n",
            "Epoch: 215, Loss: 4.332096285915019\n",
            "Epoch: 216, Loss: 4.332222552171456\n",
            "Epoch: 217, Loss: 4.332355076239452\n",
            "Epoch: 218, Loss: 4.332271077594693\n",
            "Epoch: 219, Loss: 4.3321019216281185\n",
            "Epoch: 220, Loss: 4.3320861872643075\n",
            "Epoch: 221, Loss: 4.332277507277684\n",
            "Epoch: 222, Loss: 4.332572828927296\n",
            "Epoch: 223, Loss: 4.33302224491911\n",
            "Epoch: 224, Loss: 4.333898368420507\n",
            "Epoch: 225, Loss: 4.33601252448708\n",
            "Epoch: 226, Loss: 4.338744658462647\n",
            "Epoch: 227, Loss: 4.342706119461496\n",
            "Epoch: 228, Loss: 4.341510975100969\n",
            "Epoch: 229, Loss: 4.336579460144462\n",
            "Epoch: 230, Loss: 4.332848174899645\n",
            "Epoch: 231, Loss: 4.33416228592088\n",
            "Epoch: 232, Loss: 4.337140827495921\n",
            "Epoch: 233, Loss: 4.335907026587625\n",
            "Epoch: 234, Loss: 4.3328777726848475\n",
            "Epoch: 235, Loss: 4.332697410748205\n",
            "Epoch: 236, Loss: 4.334659839957385\n",
            "Epoch: 237, Loss: 4.334965590622238\n",
            "Epoch: 238, Loss: 4.3329013707982424\n",
            "Epoch: 239, Loss: 4.332185869501325\n",
            "Epoch: 240, Loss: 4.333300393428394\n",
            "Epoch: 241, Loss: 4.333500488847245\n",
            "Epoch: 242, Loss: 4.33247482187848\n",
            "Epoch: 243, Loss: 4.332061523944048\n",
            "Epoch: 244, Loss: 4.332691710774452\n",
            "Epoch: 245, Loss: 4.332895980737721\n",
            "Epoch: 246, Loss: 4.3322128290631685\n",
            "Epoch: 247, Loss: 4.331832228528745\n",
            "Epoch: 248, Loss: 4.332150363165989\n",
            "Epoch: 249, Loss: 4.332300770249536\n",
            "Epoch: 250, Loss: 4.331992888617121\n",
            "Epoch: 251, Loss: 4.331763080974495\n",
            "Epoch: 252, Loss: 4.331885624932923\n",
            "Epoch: 253, Loss: 4.332015536989659\n",
            "Epoch: 254, Loss: 4.3318524489690935\n",
            "Epoch: 255, Loss: 4.331636723129567\n",
            "Epoch: 256, Loss: 4.3316162193342\n",
            "Epoch: 257, Loss: 4.331694206436503\n",
            "Epoch: 258, Loss: 4.33168810407588\n",
            "Epoch: 259, Loss: 4.33159369953647\n",
            "Epoch: 260, Loss: 4.331536971801044\n",
            "Epoch: 261, Loss: 4.331553569533449\n",
            "Epoch: 262, Loss: 4.331570168093067\n",
            "Epoch: 263, Loss: 4.3315402915962355\n",
            "Epoch: 264, Loss: 4.331498577395989\n",
            "Epoch: 265, Loss: 4.33146976648151\n",
            "Epoch: 266, Loss: 4.331462248723208\n",
            "Epoch: 267, Loss: 4.331457261120511\n",
            "Epoch: 268, Loss: 4.331442935533692\n",
            "Epoch: 269, Loss: 4.331421928449164\n",
            "Epoch: 270, Loss: 4.331407729644981\n",
            "Epoch: 271, Loss: 4.331412526661971\n",
            "Epoch: 272, Loss: 4.331434510104866\n",
            "Epoch: 273, Loss: 4.3314602408119605\n",
            "Epoch: 274, Loss: 4.3315029683610735\n",
            "Epoch: 275, Loss: 4.331571512060625\n",
            "Epoch: 276, Loss: 4.3317218884983\n",
            "Epoch: 277, Loss: 4.331973424465134\n",
            "Epoch: 278, Loss: 4.332395948020149\n",
            "Epoch: 279, Loss: 4.332982921031235\n",
            "Epoch: 280, Loss: 4.333807029738604\n",
            "Epoch: 281, Loss: 4.334730731233054\n",
            "Epoch: 282, Loss: 4.335561665057443\n",
            "Epoch: 283, Loss: 4.33560417804918\n",
            "Epoch: 284, Loss: 4.334532711275434\n",
            "Epoch: 285, Loss: 4.3330525247690765\n",
            "Epoch: 286, Loss: 4.332410729481646\n",
            "Epoch: 287, Loss: 4.333072959250976\n",
            "Epoch: 288, Loss: 4.334270288664233\n",
            "Epoch: 289, Loss: 4.335257907653109\n",
            "Epoch: 290, Loss: 4.334618330158289\n",
            "Epoch: 291, Loss: 4.333477704067119\n",
            "Epoch: 292, Loss: 4.332359371216292\n",
            "Epoch: 293, Loss: 4.332091220197279\n",
            "Epoch: 294, Loss: 4.33231323483494\n",
            "Epoch: 295, Loss: 4.332435662228454\n",
            "Epoch: 296, Loss: 4.332325307528161\n",
            "Epoch: 297, Loss: 4.33206595362932\n",
            "Epoch: 298, Loss: 4.3320309022193\n",
            "Epoch: 299, Loss: 4.332158216051775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(conflicts_tensor)\n",
        "tf.argmax(out, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8722c083-cdb1-4b8c-b94f-e44d0884c5af",
        "id": "wP7bxcFbI2qh"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(500, 24), dtype=int64, numpy=\n",
              "array([[23,  3, 21, ..., 17, 21,  2],\n",
              "       [17, 20, 21, ..., 11,  9, 19],\n",
              "       [15,  6, 17, ..., 11,  6,  7],\n",
              "       ...,\n",
              "       [23, 23,  2, ..., 11,  2, 23],\n",
              "       [23,  6, 17, ..., 17,  6, 14],\n",
              "       [17, 10,  7, ..., 11,  2, 23]])>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.argmax(out, 2)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6exhk5DgcdZz",
        "outputId": "0f3cbc60-1a21-44f2-c26e-bc73e524b497",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(24,), dtype=int64, numpy=\n",
              "array([23,  3, 21,  6,  8, 15,  5, 19, 15, 20, 10, 21, 19, 20, 13, 13, 13,\n",
              "       15,  8, 15, 23, 17, 21,  2])>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(tf.argmax(out, 2)[0]).size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP969BP5cYc1",
        "outputId": "6c210184-3d19-4a92-ef4e-73b5b261c5bf",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_max = tf.reduce_max(out, axis=2)\n",
        "z = tf.equal(out, tf.expand_dims(row_max, axis=2))\n",
        "q = tf.cast(z, tf.float64)\n",
        "row_max = tf.reduce_max(q, axis=1)\n",
        "penalty_dup = tf.cast(24 * 500, tf.float64) - tf.reduce_sum(row_max)\n",
        "penalty_dup = penalty_dup / tf.cast(500, tf.float64)\n",
        "penalty_dup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SagPDJ0dJJLr",
        "outputId": "2e780945-41e0-455b-f726-7382a334dddc",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=9.5>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_max"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eomN2ba9JxZP",
        "outputId": "11e90257-9bc3-4fd7-95e7-deccce17d901",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(500, 24), dtype=float64, numpy=\n",
              "array([[0., 0., 1., ..., 1., 0., 1.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 1., 1.],\n",
              "       ...,\n",
              "       [1., 1., 1., ..., 0., 1., 1.],\n",
              "       [0., 0., 1., ..., 0., 1., 1.],\n",
              "       [0., 1., 1., ..., 1., 0., 1.]])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch"
      ],
      "metadata": {
        "id": "ZkyqDVKwL5pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as op"
      ],
      "metadata": {
        "id": "URqq-PKpL68E"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeatingArr(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(SeatingArr, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense_1 = nn.Linear(24*24, 80, dtype=torch.float64)\n",
        "        self.dense_2 = nn.Linear(80, 24 * 24, dtype=torch.float64)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.dense_1(x))\n",
        "        x = F.relu(self.dense_2(x))\n",
        "        x = x.view(-1,24,24)\n",
        "        x = F.softmax(x, dim=2)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "O04p1ZE6PsS9"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = SeatingArr()\n",
        "optim = op.Adam(params=net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "SPbYGiQDTv4K"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_conflicts = torch.tensor(conflicts, dtype=torch.float64)\n",
        "loader = torch.utils.data.DataLoader(torch_conflicts, batch_size=64, pin_memory=True)\n",
        "examples = enumerate(loader)\n",
        "batch_idx, example_data = next(examples)"
      ],
      "metadata": {
        "id": "1aeJRUSSUkWV"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_idx, example_data.shape, loader.batch_size, loader.dataset.shape, len(loader.dataset), len(loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAq9gapWVOzp",
        "outputId": "0cecacea-6606-44d9-ce48-e478d7b5819f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, torch.Size([64, 24, 24]), 64, torch.Size([500, 24, 24]), 500, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AFH5gkdNY_PG",
        "outputId": "8979b507-f359-4852-c971-d27258e15926"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.DoubleTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjacent_mask = create_adjacent_mask(n_seats=24, seats_per_row=6, seats_per_col=4)\n",
        "adjacent_mask = torch.tensor(adjacent_mask, dtype=torch.float64)\n",
        "\n",
        "def calculate_conflict(seating_arrangement, conflict_matrix):\n",
        "    ca_mul = conflict_matrix * adjacent_mask\n",
        "    conflicts = torch.sum(torch.matmul(seating_arrangement.type(torch.float64), ca_mul))\n",
        "    return conflicts\n",
        "\n",
        "def custom_loss(predicted_seating_arrangement, conflicts_tensor):\n",
        "    loss = 0\n",
        "    batch_size = predicted_seating_arrangement.shape[0]\n",
        "    # Ensure the predicted seating arrangement is in float64\n",
        "    predicted_seating_arrangement = predicted_seating_arrangement.type(torch.float64)\n",
        "\n",
        "    # Calculate Conflict in produced seating arrangement\n",
        "    conflict = calculate_conflict(predicted_seating_arrangement, conflicts_tensor)\n",
        "    # Ensure each seat is assigned to only one person (columns should sum to 1)\n",
        "    col_sum_loss = torch.sum((torch.sum(predicted_seating_arrangement, dim=1) - 1) ** 2)\n",
        "\n",
        "    loss = col_sum_loss + conflict\n",
        "\n",
        "    return loss / batch_size"
      ],
      "metadata": {
        "id": "ptem726OVXAl"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model every 50 epochs\n",
        "interval = 50\n",
        "\n",
        "def Training(epoch):\n",
        "    # train\n",
        "    net.train()\n",
        "    for batch_idx, conflict in enumerate(loader):\n",
        "        optim.zero_grad()\n",
        "        output = net(conflict)\n",
        "        loss = custom_loss(output, conflict)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if epoch % interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(conflict), len(loader.dataset),\n",
        "            100. * batch_idx / len(loader), loss.item()))\n",
        "            # save state of model and optimizer\n",
        "            torch.save(net.state_dict(), '/content/model.pth')\n",
        "            torch.save(optim.state_dict(), '/content/optimizer.pth')\n"
      ],
      "metadata": {
        "id": "mkyWdiNUUGPc"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 100\n",
        "for i in range(n_epoch):\n",
        "     Training(i+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG9lI7z3b44f",
        "outputId": "fc76b251-6878-42a7-e48c-84cae0764727"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 50 [0/500 (0%)]\tLoss: 4.514548\n",
            "Train Epoch: 50 [64/500 (12%)]\tLoss: 4.403427\n",
            "Train Epoch: 50 [128/500 (25%)]\tLoss: 4.172729\n",
            "Train Epoch: 50 [192/500 (38%)]\tLoss: 4.910792\n",
            "Train Epoch: 50 [256/500 (50%)]\tLoss: 4.595813\n",
            "Train Epoch: 50 [320/500 (62%)]\tLoss: 3.963009\n",
            "Train Epoch: 50 [384/500 (75%)]\tLoss: 4.079786\n",
            "Train Epoch: 50 [364/500 (88%)]\tLoss: 4.517661\n",
            "Train Epoch: 100 [0/500 (0%)]\tLoss: 4.499898\n",
            "Train Epoch: 100 [64/500 (12%)]\tLoss: 4.394020\n",
            "Train Epoch: 100 [128/500 (25%)]\tLoss: 4.164602\n",
            "Train Epoch: 100 [192/500 (38%)]\tLoss: 4.891116\n",
            "Train Epoch: 100 [256/500 (50%)]\tLoss: 4.568378\n",
            "Train Epoch: 100 [320/500 (62%)]\tLoss: 3.946360\n",
            "Train Epoch: 100 [384/500 (75%)]\tLoss: 4.074533\n",
            "Train Epoch: 100 [364/500 (88%)]\tLoss: 4.522278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output = net(torch_conflicts)\n",
        "    loss = custom_loss(output, torch_conflicts)\n",
        "    loss_val = loss.item()\n",
        "    model_out_readable = torch.argmax(output, dim=2)"
      ],
      "metadata": {
        "id": "nC1DPugDckZ6"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQQgnIWXfj1j",
        "outputId": "5a222f62-8f3f-443d-cc30-cb4bf6a5a98e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.363660095510109"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_out_readable[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoP1gUQLfr6r",
        "outputId": "0980cbc8-14f6-4ff4-9f7f-58399866fdfb"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([14, 10, 13, 11,  9, 15, 11,  2,  7,  0,  6, 20, 13,  8, 13, 20,  0,  3,\n",
              "        19, 21, 15, 17, 22,  9])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_out_readable[0].view(6,4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhAxHqF6ftTz",
        "outputId": "faed48b4-b416-4f50-c985-5bef073686ae"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14, 10, 13, 11],\n",
              "        [ 9, 15, 11,  2],\n",
              "        [ 7,  0,  6, 20],\n",
              "        [13,  8, 13, 20],\n",
              "        [ 0,  3, 19, 21],\n",
              "        [15, 17, 22,  9]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}